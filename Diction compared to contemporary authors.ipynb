{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Diction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "corpus1 = os.listdir('Corpus')\n",
    "corpus2 = os.listdir('Contemporary_authors')\n",
    "\n",
    "corpus1 = [file for file in corpus1 if file != '.DS_Store']\n",
    "corpus2 = [file for file in corpus2 if file != '.DS_Store']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating frequencies\n",
    "\n",
    "The code in the cell below reads in the full text of the files that are listed in `corpus1`. In this case, we are dealing with one text file only. Next, we calculate the frequencies of all of these words. These frequencies are stored in a dictionary named `freq1`.\n",
    "\n",
    "Once the first subcorpus has been processed, the code does the same for the texts in `corpus2`. The word frequencies are placed in a dictionary named `freq2`.\n",
    "\n",
    "After running this code, the variable `full_text1` will contain the *complete* texts of all the texts in `corpus1`. The dictionary named `freq1` will contain the frequencies of all the words in this full text. \n",
    "\n",
    "The variables `full_text2` and `freq2` store the same type of information for the texts in `corpus2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mansfield_Park.txt ... \n",
      "Reading Sense_and_Sensibility.txt ... \n",
      "Reading Northanger_Abbey.txt ... \n",
      "Reading Persuasion.txt ... \n",
      "Reading Emma.txt ... \n",
      "Reading Pride_and_Prejudice.txt ... \n",
      "Reading 6593-0.txt ... \n",
      "Reading 3724-0.txt ... \n",
      "Reading 3409-8.txt ... \n",
      "Reading 27712.txt ... \n",
      "Reading 9539-readme.txt ... \n",
      "Reading 3719-0.txt ... \n",
      "Reading 12958.txt ... \n",
      "Reading 5826-8.txt ... \n",
      "Reading 15034-8.txt ... \n",
      "Reading 40619-8.txt ... \n",
      "Reading 3700-0.txt ... \n",
      "Reading 9525-readme.txt ... \n",
      "Reading 1147-0.txt ... \n",
      "Reading 19500-8.txt ... \n",
      "Reading 36256-8.txt ... \n",
      "Reading 7265-0.txt ... \n",
      "Reading 9534-readme.txt ... \n",
      "Reading 55212-0.txt ... \n",
      "Reading 5196-0.txt ... \n",
      "Reading 1865.txt ... \n",
      "Reading 9528-readme.txt ... \n",
      "Reading 8897.txt ... \n",
      "Reading 11936.txt ... \n",
      "Reading 619-8.txt ... \n",
      "Reading 58249-0.txt ... \n",
      "Reading 18645.txt ... \n",
      "Reading 12234.txt ... \n",
      "Reading 9531-readme.txt ... \n",
      "Reading 29000-8.txt ... \n",
      "Reading 2158.txt ... \n",
      "Reading 3767-0.txt ... \n",
      "Reading 27067.txt ... \n",
      "Reading 24103-8.txt ... \n",
      "Reading 57966-0.txt ... \n",
      "Reading 3045.txt ... \n",
      "Reading 18640-8.txt ... \n",
      "Reading 37923.txt ... \n",
      "Reading 16804-8.txt ... \n",
      "Reading 26541.txt ... \n",
      "Reading 5709-8.txt ... \n",
      "Reading 1985-0.txt ... \n",
      "Reading 30524-8.txt ... \n",
      "Reading 54424-0.txt ... \n",
      "Reading 1863.txt ... \n",
      "Reading 10558-m-readme.txt ... \n",
      "Reading 23000.txt ... \n",
      "Reading 4558-0.txt ... \n",
      "Reading 9517-readme.txt ... \n",
      "Reading 41646.txt ... \n",
      "Reading 9520-readme.txt ... \n",
      "Reading 3720-0.txt ... \n",
      "Reading 9609.txt ... \n",
      "Reading 5231.txt ... \n",
      "Reading 26287-readme.txt ... \n",
      "Reading 5256.txt ... \n",
      "Reading 42324-8.txt ... \n",
      "Reading 37774.txt ... \n",
      "Reading 9527-readme.txt ... \n",
      "Reading 36257-8.txt ... \n",
      "Reading 4969.txt ... \n",
      "Reading 54783-0.txt ... \n",
      "Reading 58558-0.txt ... \n",
      "Reading 30606-8.txt ... \n",
      "Reading 45554-8.txt ... \n",
      "Reading 599.txt ... \n",
      "Reading 24000-8.txt ... \n",
      "Reading 3718-0.txt ... \n",
      "Reading 37439-8.txt ... \n",
      "Reading 37441-8.txt ... \n",
      "Reading 2843.txt ... \n",
      "Reading 57472-0.txt ... \n",
      "Reading 9536-readme.txt ... \n",
      "Reading 1146.txt ... \n",
      "Reading 4599.txt ... \n",
      "Reading 8945-8.txt ... \n",
      "Reading 6346-0.txt ... \n",
      "Reading 3699-0.txt ... \n",
      "Reading 37438.txt ... \n",
      "Reading 5140-8.txt ... \n",
      "Reading 33500.txt ... \n",
      "Reading 9518-readme.txt ... \n",
      "Reading 9904-8.txt ... \n",
      "Reading 55665-0.txt ... \n",
      "Reading 2648.txt ... \n",
      "Reading 44992.txt ... \n",
      "Reading 9533-readme.txt ... \n",
      "Reading 29363.txt ... \n",
      "Reading 24860.txt ... \n",
      "Reading 25001.txt ... \n",
      "Reading 3723-0.txt ... \n",
      "Reading 3622.txt ... \n",
      "Reading 9540-readme.txt ... \n",
      "Reading 2686-0.txt ... \n",
      "Reading 9522-readme.txt ... \n",
      "Reading 31274.txt ... \n",
      "Reading 6098-0.txt ... \n",
      "Reading 42890.txt ... \n",
      "Reading 2844.txt ... \n",
      "Reading 30962.txt ... \n",
      "Reading 5642.txt ... \n",
      "Reading 7481-8.txt ... \n",
      "Reading 21847.txt ... \n",
      "Reading 5118.txt ... \n",
      "Reading 15766-8.txt ... \n",
      "Reading 9530-readme.txt ... \n",
      "Reading 37437.txt ... \n",
      "Reading 3616-0.txt ... \n",
      "Reading 2732.txt ... \n",
      "Reading 2646.txt ... \n",
      "Reading 20038-readme.txt ... \n",
      "Reading 13884-8.txt ... \n",
      "Reading 34922.txt ... \n",
      "Reading 2860.txt ... \n",
      "Reading 3712-0.txt ... \n",
      "Reading 2768-0.txt ... \n",
      "Reading 2675-0.txt ... \n",
      "Reading 2731.txt ... \n",
      "Reading 11889-8.txt ... \n",
      "Reading 2645.txt ... \n",
      "Reading 814.txt ... \n",
      "Reading 28676.txt ... \n",
      "Reading 3710-0.txt ... \n",
      "Reading 29964.txt ... \n",
      "Reading 5897-8.txt ... \n",
      "Reading 7152-0.txt ... \n",
      "Reading 9881.txt ... \n",
      "Reading 45921.txt ... \n",
      "Reading 3550-0.txt ... \n",
      "Reading 9521-readme.txt ... \n",
      "Reading 29828.txt ... \n",
      "Reading 3166.txt ... \n",
      "Reading 22000.txt ... \n",
      "Reading 25579.txt ... \n",
      "Reading 9538-readme.txt ... \n",
      "Reading 2511-0.txt ... \n",
      "Reading 12180-8.txt ... \n",
      "Reading 6542-readme.txt ... \n",
      "Reading 9524-readme.txt ... \n",
      "Reading 55100.txt ... \n",
      "Reading 1462-0.txt ... \n",
      "Reading 969-0.txt ... \n",
      "Reading 3716-0.txt ... \n",
      "Reading 31879-8.txt ... \n",
      "Reading 1866-8.txt ... \n",
      "Reading 9535-readme.txt ... \n",
      "Reading 26001-8.txt ... \n",
      "Reading 6828.txt ... \n",
      "Reading 2859.txt ... \n",
      "Reading 9529-readme.txt ... \n",
      "Reading 3711-0.txt ... \n",
      "Reading 6053.txt ... \n",
      "Reading 37440.txt ... \n",
      "Reading 10799.txt ... \n",
      "Reading 9519-readme.txt ... \n",
      "Reading 1933.txt ... \n",
      "Reading 10462-8.txt ... \n",
      "Reading 1990-0.txt ... \n",
      "Reading 6457.txt ... \n",
      "Reading 11643-8.txt ... \n",
      "Reading 34000.txt ... \n",
      "Reading 8123-0.txt ... \n",
      "Reading 2432.txt ... \n",
      "Reading 3615-0.txt ... \n",
      "Reading 2823-0.txt ... \n",
      "Reading 9611.txt ... \n",
      "Reading 29125.txt ... \n",
      "Reading 26002.txt ... \n",
      "Reading 58079-0.txt ... \n",
      "Reading 9541-readme.txt ... \n",
      "Reading 9798.txt ... \n",
      "Reading 897-0.txt ... \n",
      "Reading 43520-8.txt ... \n",
      "Reading 9523-readme.txt ... \n",
      "Reading 56665-0.txt ... \n",
      "Reading 27533.txt ... \n",
      "Reading 2796-0.txt ... \n",
      "Reading 19595.txt ... \n",
      "Reading 3713-0.txt ... \n",
      "Reading 55147-0.txt ... \n",
      "Reading 5202.txt ... \n",
      "Reading 6096.txt ... \n",
      "Reading 4695.txt ... \n",
      "Reading 1935.txt ... \n",
      "Reading 9526-readme.txt ... \n",
      "Reading 3717-0.txt ... \n",
      "Reading 7381-8.txt ... \n",
      "Reading 31697-8.txt ... \n",
      "Reading 18000-8.txt ... \n",
      "Reading 5978.txt ... \n",
      "Reading 6097.txt ... \n",
      "Reading 6042.txt ... \n",
      "Reading 6095.txt ... \n",
      "Reading 58383-0.txt ... \n",
      "Reading 2797.txt ... \n",
      "Reading 55648-0.txt ... \n",
      "Reading 55926-0.txt ... \n",
      "Reading 2608.txt ... \n",
      "Reading 7146-8.txt ... \n",
      "Reading 30100-8.txt ... \n",
      "Reading 28811_readme.txt ... \n",
      "Reading 19197.txt ... \n",
      "Reading 84-0.txt ... \n",
      "Reading 9537-readme.txt ... \n",
      "Reading 1969-0.txt ... \n"
     ]
    }
   ],
   "source": [
    "from tdmh import *\n",
    "from os.path import join\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "dir1 = 'Corpus'\n",
    "dir2 = 'Contemporary_authors'\n",
    "\n",
    "\n",
    "def tokenise_remove_stopwords(full_text):\n",
    "    words = word_tokenize(full_text)\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        w = w.lower().strip()\n",
    "        orig = ''\n",
    "        if w.isalnum() and w not in stopwords:\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "\n",
    "\n",
    "freq1 = dict()\n",
    "full_text1 = ''\n",
    "\n",
    "for text in corpus1:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir1,text) ) as file_handler:\n",
    "        full_text1 += file_handler.read() + ' '\n",
    "\n",
    "words = tokenise_remove_stopwords( full_text1  )\n",
    "\n",
    "for w in words:\n",
    "    freq1[w] = freq1.get(w,0) +1\n",
    "    \n",
    "       \n",
    "freq2 = dict()\n",
    "full_text2 = ''\n",
    "    \n",
    "for text in corpus2:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir2,text) ) as file_handler:\n",
    "        full_text2 += file_handler.read() + ' '\n",
    "\n",
    "words = tokenise_remove_stopwords(  full_text2 )\n",
    "\n",
    "for w in words:\n",
    "    freq2[w] = freq2.get(w,0) +1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Dunning's log likelihood\n",
    "\n",
    "One of statistical methods that can be used to find such distinctive words is *Dunning's log likelihood*. In short, it analyses the distinctiveness of word in one set of texts compared to the texts in a reference corpus, by calculating probabilities based on word frequencies. A good explanation of the fomula can be found on the [wordHoard](https://wordhoard.northwestern.edu/userman/analysis-comparewords.html#loglike) website. \n",
    "\n",
    "Using the frequencies that have been calculated above, the Dunning log likelihood scores are calculated for all of the words that occur both in `corpus1` and `corpus2` in the cell below. The actual calculation takes place in a method named `log_likelihood()`. The scores that are calculated are all stored in a dictionary named `ll_scores`\n",
    "\n",
    "The formula that is implemented in the `log_likelihood` method returns a number which can either be positive or negative. A postive score indicates that there is a high probability that the word will be used in the first corpus. A negative probability indicates that occurence of the word is more common in the second corpus. The tokens that are assigned the highest scores, in other words, are also most distincive of the first corpus. \n",
    "\n",
    "The code below lists the 25 words that are assigned a positive log likelihood score in the first corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma 4134.042121534741\n",
      "crawford 3119.821241648213\n",
      "marianne 2768.615054174358\n",
      "elizabeth 2377.4497501492524\n",
      "harriet 2251.1263099370685\n",
      "catherine 2215.965992593432\n",
      "elinor 2105.956509392492\n",
      "anne 2058.618311234011\n",
      "jane 2024.4417685894775\n",
      "elton 1978.9507151852765\n",
      "could 1941.0539434461912\n",
      "weston 1922.9396082615553\n",
      "edmund 1885.4112711323166\n",
      "fanny 1685.9947335049712\n",
      "woodhouse 1565.1085790505601\n",
      "bertram 1379.91562154255\n",
      "elliot 1365.2143203398275\n",
      "bingley 1200.3737595082744\n",
      "jennings 1151.4434134133244\n",
      "tilney 1114.8439131072666\n",
      "fairfax 1104.9407725940644\n",
      "norris 983.2512245559708\n",
      "wickham 957.6162553281514\n",
      "sister 919.049837553523\n",
      "bennet 886.7744885919558\n",
      "must 878.6634980331087\n",
      "mansfield 855.6334922815815\n",
      "feelings 840.5327587472831\n",
      "thomas 821.2384005075502\n",
      "edward 814.4726521507177\n",
      "willoughby 811.9225731382959\n",
      "isabella 800.6203522768513\n",
      "churchill 800.039603634581\n",
      "collins 749.1563067824953\n",
      "morland 748.5563347843636\n",
      "thorpe 715.9695143861669\n",
      "soon 713.4528647798059\n",
      "brandon 691.4021327625734\n",
      "lydia 677.6493486298284\n",
      "bates 644.0900290519962\n",
      "allen 642.4305913247783\n",
      "miss 627.8115549057741\n",
      "highbury 585.9315233392216\n",
      "ferrars 570.4379116098875\n",
      "quite 529.604900783608\n",
      "nothing 521.4302153945298\n",
      "henry 483.48890020924966\n",
      "gardiner 481.98048331752216\n",
      "every 478.84931011485173\n",
      "lucy 476.5863424296465\n",
      "felt 470.9449113617966\n",
      "really 457.3456014794921\n",
      "lizzy 438.20081574250946\n",
      "feel 401.2398365251255\n",
      "russell 379.3677729961638\n",
      "might 377.91342497609776\n",
      "middleton 367.6919216275689\n",
      "much 363.78523138363084\n",
      "perry 363.14864748212904\n",
      "eleanor 361.7560071564537\n",
      "yates 330.6404306050904\n",
      "think 324.80090803847474\n",
      "grant 323.2276357433225\n",
      "replied 319.84872894316584\n",
      "attachment 316.5013999651727\n",
      "bath 309.7473201217683\n",
      "louisa 303.1274123282907\n",
      "exactly 302.2177737139973\n",
      "sisters 299.15095287112604\n",
      "thing 289.8432301400627\n",
      "walter 287.0357857758332\n",
      "everything 286.51490330667013\n",
      "attention 280.97838504086076\n",
      "lyme 276.02617403605086\n",
      "cole 275.0868930547448\n",
      "spirits 273.0420749542802\n",
      "engagement 272.324236585761\n",
      "certainly 271.1229186654938\n",
      "julia 267.1324447993624\n",
      "mary 263.12390728783106\n",
      "frank 257.68143126151074\n",
      "minutes 255.03447545092064\n",
      "something 254.83046510687257\n",
      "oh 253.36315188805554\n",
      "perfectly 248.39043843963037\n",
      "would 245.9457294502572\n",
      "comfort 244.77305351219178\n",
      "yes 244.7510306325891\n",
      "illustration 241.07487737692384\n",
      "attentions 240.74491838325952\n",
      "kitty 238.81588030969152\n",
      "william 234.80779919977812\n",
      "acquaintance 233.908465331137\n",
      "sure 232.24778213755383\n",
      "directly 230.85754011328564\n",
      "agreeable 229.1736770140094\n",
      "charles 228.18672031681353\n",
      "palmer 227.03654884816507\n",
      "settled 223.30254289793706\n",
      "talked 222.79007341602522\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def log_likelihood( word_count1 , word_count2, total1 , total_2 ):\n",
    "\n",
    "    a = word_count1\n",
    "    b = word_count2\n",
    "    c = total1\n",
    "    d = total2\n",
    " \n",
    "    perc1 = (a/c)*100\n",
    "    perc2 = (b/d)*100\n",
    "    polarity = perc1 - perc2\n",
    " \n",
    "    E1 = c*(a+b)/(c+d)\n",
    "    E2 = d*(a+b)/(c+d)\n",
    "    \n",
    "    ln1 = math.log(a/E1)\n",
    "    ln2 = math.log(b/E2)\n",
    "    G2 = 2*((a* ln1) + (b* ln2))\n",
    "    \n",
    "    #if polarity < 0:\n",
    "    #    G2 = -G2\n",
    "    if a * math.log(a / E1) < 0:\n",
    "        G2 = -G2\n",
    "\n",
    "    return G2\n",
    "\n",
    "\n",
    "\n",
    "ll_scores = dict()\n",
    "\n",
    "total1 = 0\n",
    "total2 = 0\n",
    "\n",
    "for word1 in freq1:\n",
    "    total1 += freq1[word1]\n",
    "for word2 in freq2:\n",
    "    total2 += freq2[word2]\n",
    "\n",
    "for word in freq1:\n",
    "    if word in freq2:\n",
    "\n",
    "        ll_score = log_likelihood( freq1[word] , freq2[word] , total1 , total2 )\n",
    "        ll_scores[word] = ll_score\n",
    "\n",
    "max = 100\n",
    "i = 0 \n",
    "        \n",
    "for word in sortedByValue(ll_scores , ascending = False ):\n",
    "    print( word , ll_scores[word] )\n",
    "    i += 1\n",
    "    if i == max: \n",
    "        break        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words with negative log likelihood scores are more likely to appear in the reference corpus (i.e. the second corpus) than in the first corpus. \n",
    "\n",
    "The code below lists the 25 words with the highest negative scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lord -698.1556430971129\n",
      "upon -542.5912241396121\n",
      "pen -491.6974240497365\n",
      "thou -473.3400254776525\n",
      "says -469.7952064197819\n",
      "old -455.70473376902623\n",
      "king -431.3928879484596\n",
      "cries -361.3240279370904\n",
      "cecilia -352.92091719867125\n",
      "thy -319.0388343844559\n",
      "doctor -314.84138013692973\n",
      "amelia -306.0648701810853\n",
      "madame -303.12631218735066\n",
      "harry -295.58331812878777\n",
      "george -288.828522844759\n",
      "queen -286.1196607849365\n",
      "camilla -283.70029070455854\n",
      "jones -276.8773515145193\n",
      "de -270.200649245787\n",
      "prince -269.40795848802827\n",
      "french -260.29246394239703\n",
      "madam -257.63997386971243\n",
      "arthur -253.8320525385486\n",
      "gentleman -202.3464406613511\n",
      "men -200.5650638737169\n",
      "reader -194.2371291469019\n",
      "boy -192.6068291360287\n",
      "english -185.9888479649926\n",
      "man -179.0260632696793\n",
      "master -177.35092028224742\n",
      "duke -161.24449219029145\n",
      "money -160.65950961569075\n",
      "ellis -156.8358490018687\n",
      "fellow -154.99964303940024\n",
      "god -151.45244034936212\n",
      "answered -150.779083252045\n",
      "life -146.3837399841712\n",
      "art -146.2936672967848\n",
      "scarce -145.43715355262813\n",
      "arms -138.93770414771444\n",
      "court -138.29065157913692\n",
      "lordship -134.94136541319938\n",
      "city -131.83275712169552\n",
      "author -126.13305651506545\n",
      "page -122.97604807453374\n",
      "honest -120.22407280298965\n",
      "passion -119.65344368554113\n",
      "laura -118.9799188528468\n",
      "order -117.74524243241162\n",
      "wretch -117.69648392860095\n",
      "thus -117.49662305817657\n",
      "roman -115.17213618560433\n",
      "heaven -113.68575593863162\n",
      "religion -112.5821380581342\n",
      "hand -111.7105174162661\n",
      "army -111.03028007144262\n",
      "sophia -109.74127403819796\n",
      "death -109.5016254970223\n",
      "nevertheless -102.8600739593372\n",
      "france -102.5758621967301\n",
      "black -102.45410785261296\n",
      "parson -101.75614354026048\n",
      "noble -101.6079247175698\n",
      "us -101.00418902163574\n",
      "battle -99.89679957128767\n",
      "feet -99.56419836776541\n",
      "lad -98.11280486208209\n",
      "yet -97.85732491903889\n",
      "story -97.61728699126252\n",
      "worthy -97.58379415852595\n",
      "enemy -97.08020427722985\n",
      "honor -95.34670338390788\n",
      "soul -94.624278725378\n",
      "johnson -94.32825885888411\n",
      "wicked -93.70099094009471\n",
      "honour -92.98922850534632\n",
      "caesar -92.79125947464209\n",
      "edgar -92.64609202236329\n",
      "virtue -91.68779305797923\n",
      "countess -90.86849933604904\n",
      "although -89.9300910973522\n",
      "poor -89.72008964175649\n",
      "war -89.40834078430665\n",
      "caused -88.95116529531944\n",
      "la -87.32802131605322\n",
      "england -85.55310178528549\n",
      "hands -84.88467159443312\n",
      "innocent -82.65235543385771\n",
      "face -82.49397304766299\n",
      "glass -81.81959471629077\n",
      "ii -79.39119971669163\n",
      "bishop -76.35373466179104\n",
      "iii -74.56917261554159\n",
      "history -74.27101935003397\n",
      "brave -73.90085090474172\n",
      "flung -72.57138117133394\n",
      "minister -72.57138117133394\n",
      "whilst -72.37358671727918\n",
      "betty -71.26314248030073\n",
      "mamma -70.6462087515721\n"
     ]
    }
   ],
   "source": [
    "max = 100\n",
    "i = 0 \n",
    "\n",
    "for word in sortedByValue(ll_scores ) :\n",
    "    print( word , ll_scores[word] )\n",
    "    i += 1\n",
    "    if i == max:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann Whitney formula\n",
    "\n",
    "In a [blogpost on identifying literary diction](https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/), Ted Underwood argues that Dunning's log likelihood function also has a number of disadvantages. It is sensitive to outliers, for example. He explains that the Mann Whitney ranks test can be a good alternative. \n",
    "\n",
    "To perform the Mann-Whitney ranks test, we firstly need to find all the words the two corpora to be compared have in common. Next, we need to divide the full texts of the two corpora to be compared into smaller chuncks, all of the same size. These can be fragments of 500 words, for instance. Next, we need to count the number of times each word occurs in these chunks. Using these counts, we can determine whether the word is more frequent in corpus 1 than in corpus 2 (or vice versa). As a final step, we determine the total number of fragments in which the word is most frequent, both in the first and the second corpus. If it is found, using these steps, that a word is much more common in one of the two corpora, this word can be identified as a distinctive word. The Mann-Whitney ranks test really looks at occurrences across the whole corpus, and it is neutralises the effect of exceptionally high counts in one or two of these chunks.      \n",
    "\n",
    "The Mann Whitney test can be performed in Python using the `mannwhitneyu()` method from the `scipy.stats` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "## make a list of all the words in both corpora\n",
    "words1 = tokenise_remove_stopwords(full_text1)\n",
    "words2 = tokenise_remove_stopwords(full_text2)\n",
    "\n",
    "def divide_into_chunks(words, length):\n",
    "\n",
    "    chunks=[]\n",
    "    ## chunk contains dictionaries\n",
    "    # with word frequencies\n",
    "    \n",
    "    for i in range(0, len(words), length):\n",
    "        counts = dict()\n",
    "        for j in range(length):\n",
    "            if i+j < len(words):\n",
    "                word = words[i+j]\n",
    "                counts[word] = counts.get(word,0)+1\n",
    "        chunks.append(counts)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "length = 500\n",
    "chunks1 = divide_into_chunks(words1,length)\n",
    "chunks2 = divide_into_chunks(words2,length)\n",
    "\n",
    "\n",
    "# vocab is the union of terms in both sets\n",
    "all_words = dict()\n",
    "    \n",
    "for chunk in chunks1:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "for chunk in chunks2:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "    \n",
    "rho =  dict()\n",
    "    \n",
    "for word in all_words:\n",
    "        \n",
    "    a=[]\n",
    "    b=[]\n",
    "        \n",
    "    for chunk in chunks1:\n",
    "        a.append(chunk.get(word,0))\n",
    "    for chunk in chunks2:\n",
    "        b.append(chunk.get(word,0))\n",
    "\n",
    "    stat,pval=mannwhitneyu(a,b, alternative=\"two-sided\")\n",
    "    mean =len(chunks1)*len(chunks2)*0.5\n",
    "    if stat <= mean:\n",
    "        pval = 0 - pval\n",
    "            \n",
    "    rho[word]= ( pval )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are most distinctive in corpus 1 have a negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following words are most distinctive in corpus 1:\n",
      "woodhouse\t0.0000000000000000000000\n",
      "knightley\t0.0000000000000000000000\n",
      "bertram\t0.0000000000000000000000\n",
      "dashwood\t0.0000000000000000000000\n",
      "marianne\t0.0000000000000000000000\n",
      "darcy\t0.0000000000000000000000\n",
      "edmund\t0.0000000000000000000000\n",
      "elton\t0.0000000000000000000000\n",
      "mansfield\t0.0000000000000000000000\n",
      "hartfield\t0.0000000000000000000000\n",
      "weston\t0.0000000000000000000000\n",
      "jane\t0.0000000000000000000000\n",
      "feelings\t0.0000000000000000000000\n",
      "highbury\t0.0000000000000000000000\n",
      "fairfax\t0.0000000000000000000000\n",
      "could\t0.0000000000000000000000\n",
      "harriet\t0.0000000000000000000000\n",
      "rushworth\t0.0000000000000000000000\n",
      "norris\t0.0000000000000000000000\n",
      "jennings\t0.0000000000000000000000\n",
      "tilney\t0.0000000000000000000000\n",
      "catherine\t0.0000000000000000000000\n",
      "barton\t0.0000000000000000000000\n",
      "wickham\t0.0000000000000000000000\n",
      "longbourn\t0.0000000000000000000000\n",
      "isabella\t0.0000000000000000000000\n",
      "brandon\t0.0000000000000000000000\n",
      "wentworth\t0.0000000000000000000000\n",
      "sister\t0.0000000000000000000000\n",
      "elliot\t0.0000000000000000000000\n",
      "randalls\t0.0000000000000000000000\n",
      "musgrove\t0.0000000000000000000000\n",
      "morland\t0.0000000000000000000000\n",
      "elinor\t0.0000000000000000000000\n",
      "elizabeth\t0.0000000000000000000000\n",
      "bates\t0.0000000000000000000000\n",
      "churchill\t0.0000000000000000000000\n",
      "netherfield\t0.0000000000000000000000\n",
      "anne\t0.0000000000000000000000\n",
      "soon\t0.0000000000000000000000\n",
      "meryton\t0.0000000000000000000000\n",
      "lizzy\t0.0000000000000000000000\n",
      "bingley\t0.0000000000000000000000\n",
      "must\t0.0000000000000000000000\n",
      "uppercross\t0.0000000000000000000000\n",
      "ferrars\t0.0000000000000000000000\n",
      "really\t0.0000000000000000000000\n",
      "kellynch\t0.0000000000000000000000\n",
      "thorpe\t0.0000000000000000000000\n",
      "felt\t0.0000000000000000000000\n",
      "bennet\t0.0000000000000000000000\n",
      "exactly\t0.0000000000000000000000\n",
      "lydia\t0.0000000000000000000000\n",
      "feel\t0.0000000000000000000000\n",
      "attachment\t0.0000000000000000000000\n",
      "pemberley\t0.0000000000000000000000\n",
      "nothing\t0.0000000000000000000000\n",
      "allen\t0.0000000000000000000000\n",
      "donwell\t0.0000000000000000000000\n",
      "gardiner\t0.0000000000000000000000\n",
      "attentions\t0.0000000000000000000000\n",
      "collins\t0.0000000000000000000000\n",
      "dashwoods\t0.0000000000000000000000\n",
      "miss\t0.0000000000000000000000\n",
      "attention\t0.0000000000000000000000\n",
      "rosings\t0.0000000000000000000000\n",
      "replied\t0.0000000000000000000000\n",
      "quite\t0.0000000000000000000000\n",
      "benwick\t0.0000000000000000000000\n",
      "norland\t0.0000000000000000000000\n",
      "fanny\t0.0000000000000000000000\n",
      "yates\t0.0000000000000000000000\n",
      "bertrams\t0.0000000000000000000000\n",
      "directly\t0.0000000000000000000000\n",
      "minutes\t0.0000000000000000000000\n",
      "musgroves\t0.0000000000000000000000\n",
      "sotherton\t0.0000000000000000000000\n",
      "harville\t0.0000000000000000000000\n",
      "perry\t0.0000000000000000000000\n",
      "willoughby\t0.0000000000000000000000\n",
      "perfectly\t0.0000000000000000000000\n",
      "spirits\t0.0000000000000000000000\n",
      "certainly\t0.0000000000000000000000\n",
      "croft\t0.0000000000000000000000\n",
      "talked\t0.0000000000000000000000\n",
      "hertfordshire\t0.0000000000000000000000\n",
      "eleanor\t0.0000000000000000000000\n",
      "settled\t0.0000000000000000000000\n",
      "much\t0.0000000000000000000000\n",
      "julia\t0.0000000000000000000000\n",
      "cole\t0.0000000000000000000000\n",
      "enscombe\t0.0000000000000000000000\n",
      "bourgh\t0.0000000000000000000000\n",
      "every\t0.0000000000000000000000\n",
      "sisters\t0.0000000000000000000000\n",
      "cousins\t0.0000000000000000000000\n",
      "parsonage\t0.0000000000000000000000\n",
      "farther\t0.0000000000000000000000\n",
      "might\t0.0000000000000000000000\n",
      "oh\t0.0000000000000000000000\n",
      "everything\t0.0000000000000000000000\n",
      "middleton\t0.0000000000000000000000\n",
      "aunt\t0.0000000000000000000000\n",
      "subject\t0.0000000000000000000000\n",
      "middletons\t0.0000000000000000000000\n",
      "henry\t0.0000000000000000000000\n",
      "engagement\t0.0000000000000000000000\n",
      "beyond\t0.0000000000000000000000\n",
      "comfort\t0.0000000000000000000000\n",
      "fullerton\t0.0000000000000000000000\n",
      "northanger\t0.0000000000000000000000\n",
      "edward\t0.0000000000000000000000\n",
      "something\t0.0000000000000000000000\n",
      "seemed\t0.0000000000000000000000\n",
      "hunsford\t0.0000000000000000000000\n",
      "delaford\t0.0000000000000000000000\n",
      "think\t0.0000000000000000000000\n",
      "thomas\t0.0000000000000000000000\n",
      "bath\t0.0000000000000000000000\n",
      "manners\t0.0000000000000000000000\n",
      "camden\t0.0000000000000000000000\n",
      "particularly\t0.0000000000000000000000\n",
      "agreeable\t0.0000000000000000000000\n",
      "decided\t0.0000000000000000000000\n",
      "happy\t0.0000000000000000000000\n",
      "affection\t0.0000000000000000000000\n",
      "acquaintance\t0.0000000000000000000000\n",
      "hayter\t0.0000000000000000000000\n",
      "derbyshire\t0.0000000000000000000000\n",
      "happiness\t0.0000000000000000000000\n",
      "excepting\t0.0000000000000000000000\n",
      "lyme\t0.0000000000000000000000\n",
      "smallest\t0.0000000000000000000000\n",
      "goddard\t0.0000000000000000000000\n",
      "enjoyment\t0.0000000000000000000000\n",
      "yes\t0.0000000000000000000000\n",
      "idea\t0.0000000000000000000000\n",
      "eltons\t0.0000000000000000000000\n",
      "campbells\t0.0000000000000000000000\n",
      "differently\t0.0000000000000000000000\n",
      "hardly\t0.0000000000000000000000\n",
      "enough\t0.0000000000000000000000\n",
      "lucas\t0.0000000000000000000000\n",
      "wishing\t0.0000000000000000000000\n",
      "tilneys\t0.0000000000000000000000\n",
      "steeles\t0.0000000000000000000000\n",
      "pianoforte\t0.0000000000000000000000\n",
      "hurst\t0.0000000000000000000000\n",
      "inclination\t0.0000000000000000000000\n",
      "materially\t0.0000000000000000000000\n",
      "lucy\t0.0000000000000000000000\n",
      "satisfied\t0.0000000000000000000000\n",
      "solicitude\t0.0000000000000000000000\n",
      "composure\t0.0000000000000000000000\n",
      "comprehend\t0.0000000000000000000000\n",
      "conviction\t0.0000000000000000000000\n",
      "allenham\t0.0000000000000000000000\n",
      "claims\t0.0000000000000000000000\n",
      "dalrymple\t0.0000000000000000000000\n",
      "milsom\t0.0000000000000000000000\n",
      "feeling\t0.0000000000000000000000\n",
      "kitty\t0.0000000000000000000000\n",
      "without\t0.0000000000000000000000\n",
      "wanted\t0.0000000000000000000000\n",
      "half\t0.0000000000000000000000\n",
      "equal\t0.0000000000000000000000\n",
      "louisa\t0.0000000000000000000000\n",
      "distressing\t0.0000000000000000000000\n",
      "immediately\t0.0000000000000000000000\n",
      "mama\t0.0000000000000000000000\n",
      "talking\t0.0000000000000000000000\n",
      "campbell\t0.0000000000000000000000\n",
      "wish\t0.0000000000000000000000\n",
      "believed\t0.0000000000000000000000\n",
      "staying\t0.0000000000000000000000\n",
      "foreseen\t0.0000000000000000000000\n",
      "sure\t0.0000000000000000000000\n",
      "pulteney\t0.0000000000000000000000\n",
      "crawfords\t0.0000000000000000000000\n",
      "woodston\t0.0000000000000000000000\n",
      "able\t0.0000000000000000000000\n",
      "northampton\t0.0000000000000000000000\n",
      "agitation\t0.0000000000000000000000\n",
      "else\t0.0000000000000000000000\n",
      "sensible\t0.0000000000000000000000\n",
      "spite\t0.0000000000000000000000\n",
      "deal\t0.0000000000000000000000\n",
      "assure\t0.0000000000000000000000\n",
      "abbey\t0.0000000000000000000000\n",
      "anybody\t0.0000000000000000000000\n",
      "philips\t0.0000000000000000000000\n",
      "convinced\t0.0000000000000000000000\n",
      "degree\t0.0000000000000000000000\n",
      "mary\t0.0000000000000000000000\n",
      "trying\t0.0000000000000000000000\n",
      "anything\t0.0000000000000000000000\n",
      "anxious\t0.0000000000000000000000\n",
      "possible\t0.0000000000000000000000\n",
      "wallis\t0.0000000000000000000000\n",
      "northamptonshire\t0.0000000000000000000000\n",
      "dixon\t0.0000000000000000000000\n",
      "connexions\t0.0000000000000000000000\n",
      "scarcely\t0.0000000000000000000000\n",
      "thorpes\t0.0000000000000000000000\n",
      "everingham\t0.0000000000000000000000\n",
      "lucases\t0.0000000000000000000000\n",
      "thornton\t0.0000000000000000000000\n",
      "harvilles\t0.0000000000000000000000\n",
      "towards\t0.0000000000000000000000\n",
      "cleveland\t0.0000000000000000000000\n",
      "always\t0.0000000000000000000000\n",
      "removal\t0.0000000000000000000000\n",
      "consciousness\t0.0000000000000000000000\n",
      "persuaded\t0.0000000000000000000000\n",
      "possibility\t0.0000000000000000000000\n",
      "pleasure\t0.0000000000000000000000\n",
      "material\t0.0000000000000000000000\n",
      "indifference\t0.0000000000000000000000\n",
      "connexion\t0.0000000000000000000000\n",
      "situation\t0.0000000000000000000000\n",
      "would\t0.0000000000000000000000\n",
      "desirable\t0.0000000000000000000000\n",
      "wanting\t0.0000000000000000000000\n",
      "portsmouth\t0.0000000000000000000000\n",
      "addition\t0.0000000000000000000000\n",
      "talker\t0.0000000000000000000000\n",
      "civility\t0.0000000000000000000000\n",
      "seeing\t0.0000000000000000000000\n",
      "evening\t0.0000000000000000000000\n",
      "ought\t0.0000000000000000000000\n",
      "likely\t0.0000000000000000000000\n",
      "importance\t0.0000000000000000000000\n",
      "thing\t0.0000000000000000000000\n",
      "westons\t0.0000000000000000000000\n",
      "bennets\t0.0000000000000000000000\n",
      "inconvenience\t0.0000000000000000000000\n",
      "longer\t0.0000000000000000000000\n",
      "forster\t0.0000000000000000000000\n",
      "rational\t0.0000000000000000000000\n",
      "object\t0.0000000000000000000000\n",
      "walked\t0.0000000000000000000000\n",
      "depend\t0.0000000000000000000000\n",
      "dependence\t0.0000000000000000000000\n",
      "party\t0.0000000000000000000000\n",
      "influence\t0.0000000000000000000000\n",
      "wished\t0.0000000000000000000000\n",
      "walking\t0.0000000000000000000000\n",
      "fitzwilliam\t0.0000000000000000000000\n",
      "moment\t0.0000000000000000000000\n",
      "improvement\t0.0000000000000000000000\n",
      "aware\t0.0000000000000000000000\n",
      "interest\t0.0000000000000000000000\n",
      "general\t0.0000000000000000000000\n",
      "comfortable\t0.0000000000000000000000\n",
      "circumstance\t0.0000000000000000000000\n",
      "however\t0.0000000000000000000000\n",
      "hope\t0.0000000000000000000000\n",
      "eldest\t0.0000000000000000000000\n",
      "well\t0.0000000000000000000000\n",
      "unpleasant\t0.0000000000000000000000\n",
      "aye\t0.0000000000000000000000\n",
      "calmness\t0.0000000000000000000000\n",
      "sort\t0.0000000000000000000000\n",
      "impossible\t0.0000000000000000000000\n",
      "advantage\t0.0000000000000000000000\n",
      "lambton\t0.0000000000000000000000\n",
      "coles\t0.0000000000000000000000\n",
      "pratt\t0.0000000000000000000000\n",
      "adieus\t0.0000000000000000000000\n",
      "allens\t0.0000000000000000000000\n",
      "glad\t0.0000000000000000000000\n",
      "reasonable\t0.0000000000000000000000\n",
      "difference\t0.0000000000000000000000\n",
      "unreserve\t0.0000000000000000000000\n",
      "palmer\t0.0000000000000000000000\n",
      "understand\t0.0000000000000000000001\n",
      "elegance\t0.0000000000000000000001\n",
      "entirely\t0.0000000000000000000001\n",
      "maria\t0.0000000000000000000001\n",
      "exceedingly\t0.0000000000000000000001\n",
      "pain\t0.0000000000000000000002\n",
      "decidedly\t0.0000000000000000000002\n",
      "supposed\t0.0000000000000000000002\n",
      "together\t0.0000000000000000000002\n",
      "pause\t0.0000000000000000000002\n",
      "grant\t0.0000000000000000000002\n",
      "mistaken\t0.0000000000000000000002\n",
      "clay\t0.0000000000000000000002\n",
      "disposition\t0.0000000000000000000002\n",
      "supposing\t0.0000000000000000000003\n",
      "coming\t0.0000000000000000000003\n",
      "betsey\t0.0000000000000000000004\n",
      "exploring\t0.0000000000000000000004\n",
      "churchills\t0.0000000000000000000004\n",
      "anxiety\t0.0000000000000000000004\n",
      "never\t0.0000000000000000000005\n",
      "persuade\t0.0000000000000000000007\n",
      "william\t0.0000000000000000000008\n",
      "twelvemonth\t0.0000000000000000000008\n",
      "fancying\t0.0000000000000000000010\n",
      "shrubbery\t0.0000000000000000000014\n",
      "walter\t0.0000000000000000000015\n",
      "crofts\t0.0000000000000000000016\n",
      "chuse\t0.0000000000000000000023\n",
      "hour\t0.0000000000000000000025\n",
      "knightleys\t0.0000000000000000000034\n",
      "anhalt\t0.0000000000000000000034\n",
      "martins\t0.0000000000000000000034\n",
      "attending\t0.0000000000000000000035\n",
      "less\t0.0000000000000000000035\n",
      "gracechurch\t0.0000000000000000000040\n",
      "sorry\t0.0000000000000000000045\n",
      "point\t0.0000000000000000000083\n",
      "invitation\t0.0000000000000000000094\n",
      "curiosity\t0.0000000000000000000095\n",
      "chapter\t0.0000000000000000000100\n",
      "allow\t0.0000000000000000000108\n",
      "expectation\t0.0000000000000000000110\n",
      "considering\t0.0000000000000000000112\n",
      "marrying\t0.0000000000000000000116\n",
      "obliged\t0.0000000000000000000116\n",
      "listened\t0.0000000000000000000118\n",
      "uncomfortable\t0.0000000000000000000122\n",
      "weather\t0.0000000000000000000197\n",
      "tolerably\t0.0000000000000000000201\n",
      "added\t0.0000000000000000000204\n",
      "beginning\t0.0000000000000000000234\n",
      "neighbourhood\t0.0000000000000000000241\n",
      "irritation\t0.0000000000000000000376\n",
      "suppose\t0.0000000000000000000411\n",
      "time\t0.0000000000000000000476\n",
      "hoped\t0.0000000000000000000663\n",
      "pleasing\t0.0000000000000000000837\n",
      "frank\t0.0000000000000000000926\n",
      "unwelcome\t0.0000000000000000001138\n",
      "delightful\t0.0000000000000000001178\n",
      "speaking\t0.0000000000000000001707\n",
      "thoroughly\t0.0000000000000000002333\n",
      "probability\t0.0000000000000000003081\n",
      "smith\t0.0000000000000000003829\n",
      "excessively\t0.0000000000000000004432\n",
      "short\t0.0000000000000000004557\n",
      "others\t0.0000000000000000008097\n",
      "wishes\t0.0000000000000000008196\n",
      "understanding\t0.0000000000000000008518\n",
      "regret\t0.0000000000000000009148\n",
      "animation\t0.0000000000000000010089\n",
      "allowable\t0.0000000000000000013120\n",
      "side\t0.0000000000000000013173\n",
      "different\t0.0000000000000000013308\n",
      "illustration\t0.0000000000000000014596\n",
      "done\t0.0000000000000000018195\n",
      "inquiry\t0.0000000000000000020243\n",
      "rushworths\t0.0000000000000000021644\n",
      "accustomary\t0.0000000000000000021644\n",
      "bartlett\t0.0000000000000000021644\n",
      "gardiners\t0.0000000000000000021644\n",
      "smallridge\t0.0000000000000000021644\n",
      "cobb\t0.0000000000000000023002\n",
      "antigua\t0.0000000000000000023034\n",
      "wimpole\t0.0000000000000000023034\n",
      "completely\t0.0000000000000000024692\n",
      "dare\t0.0000000000000000024912\n",
      "warmly\t0.0000000000000000026306\n",
      "real\t0.0000000000000000033041\n",
      "walk\t0.0000000000000000056087\n",
      "consequence\t0.0000000000000000065661\n",
      "weeks\t0.0000000000000000070665\n",
      "consider\t0.0000000000000000072625\n",
      "felicity\t0.0000000000000000084495\n",
      "everybody\t0.0000000000000000088893\n",
      "thorough\t0.0000000000000000111209\n",
      "admiral\t0.0000000000000000112789\n",
      "recollection\t0.0000000000000000125367\n",
      "engagements\t0.0000000000000000132122\n",
      "compliment\t0.0000000000000000135066\n",
      "increasing\t0.0000000000000000142257\n",
      "astonishment\t0.0000000000000000142664\n",
      "unconnected\t0.0000000000000000167663\n",
      "kindness\t0.0000000000000000183665\n",
      "encouragement\t0.0000000000000000203960\n",
      "persuasion\t0.0000000000000000226402\n",
      "attach\t0.0000000000000000245898\n",
      "carriage\t0.0000000000000000245953\n",
      "depended\t0.0000000000000000272961\n",
      "particular\t0.0000000000000000275699\n",
      "independence\t0.0000000000000000287865\n",
      "anyone\t0.0000000000000000288853\n",
      "russell\t0.0000000000000000309951\n",
      "intimacy\t0.0000000000000000332586\n",
      "home\t0.0000000000000000376653\n",
      "morrow\t0.0000000000000000390992\n",
      "warmth\t0.0000000000000000416515\n",
      "away\t0.0000000000000000430075\n",
      "respectable\t0.0000000000000000508668\n",
      "exertion\t0.0000000000000000616232\n",
      "better\t0.0000000000000000685220\n",
      "civilities\t0.0000000000000000845112\n",
      "remarkably\t0.0000000000000000961007\n",
      "fortunate\t0.0000000000000001012319\n",
      "steady\t0.0000000000000001219188\n",
      "first\t0.0000000000000001240915\n",
      "judgment\t0.0000000000000001241493\n",
      "praise\t0.0000000000000001478328\n",
      "change\t0.0000000000000001500366\n",
      "determined\t0.0000000000000001516424\n",
      "brother\t0.0000000000000001549990\n",
      "grandmama\t0.0000000000000002038793\n",
      "long\t0.0000000000000002198010\n",
      "interference\t0.0000000000000002432515\n",
      "match\t0.0000000000000002490804\n",
      "bye\t0.0000000000000003171574\n",
      "expected\t0.0000000000000003652388\n",
      "sitting\t0.0000000000000004140853\n",
      "delighted\t0.0000000000000004218595\n",
      "interesting\t0.0000000000000004665219\n",
      "smile\t0.0000000000000004963797\n",
      "moments\t0.0000000000000005294415\n",
      "surprise\t0.0000000000000006085202\n",
      "within\t0.0000000000000007909955\n",
      "whenever\t0.0000000000000008205025\n",
      "giving\t0.0000000000000008563939\n",
      "glow\t0.0000000000000010166454\n",
      "amiable\t0.0000000000000011381031\n",
      "carteret\t0.0000000000000012991009\n",
      "eligibility\t0.0000000000000013062580\n",
      "dislike\t0.0000000000000013504256\n",
      "stornaway\t0.0000000000000013979212\n",
      "doubtingly\t0.0000000000000013979212\n",
      "elliots\t0.0000000000000013979212\n",
      "bingleys\t0.0000000000000013979212\n",
      "ecclesford\t0.0000000000000013979236\n",
      "collinses\t0.0000000000000013979236\n",
      "longstaple\t0.0000000000000013979248\n",
      "bragge\t0.0000000000000013979248\n",
      "laconia\t0.0000000000000013979251\n",
      "style\t0.0000000000000013996837\n",
      "observing\t0.0000000000000019619336\n",
      "notice\t0.0000000000000020555396\n",
      "family\t0.0000000000000023040851\n",
      "undoubtedly\t0.0000000000000023554620\n",
      "opinion\t0.0000000000000027237595\n",
      "gratifying\t0.0000000000000029400852\n",
      "thinking\t0.0000000000000030040245\n",
      "park\t0.0000000000000030088762\n",
      "encouraging\t0.0000000000000030307961\n",
      "bustle\t0.0000000000000030353798\n",
      "going\t0.0000000000000030363437\n",
      "increase\t0.0000000000000034807052\n",
      "cheerfulness\t0.0000000000000049280487\n",
      "gentlemanlike\t0.0000000000000062055309\n",
      "often\t0.0000000000000064744232\n",
      "occur\t0.0000000000000070932429\n",
      "either\t0.0000000000000082575876\n",
      "attached\t0.0000000000000102058878\n",
      "unnecessary\t0.0000000000000106212019\n",
      "meant\t0.0000000000000125222115\n",
      "visitor\t0.0000000000000130443364\n",
      "satisfaction\t0.0000000000000130903391\n",
      "disposed\t0.0000000000000135753227\n",
      "arrangement\t0.0000000000000146602628\n",
      "cottage\t0.0000000000000163784935\n",
      "serious\t0.0000000000000185294903\n",
      "afraid\t0.0000000000000192551312\n",
      "henrietta\t0.0000000000000213228092\n",
      "denny\t0.0000000000000213796255\n",
      "niece\t0.0000000000000238867669\n",
      "event\t0.0000000000000257051637\n",
      "totally\t0.0000000000000266307636\n",
      "increased\t0.0000000000000277234010\n",
      "alarm\t0.0000000000000278418004\n",
      "hearing\t0.0000000000000344356792\n",
      "secrecy\t0.0000000000000377008223\n",
      "resolution\t0.0000000000000385088024\n",
      "obliging\t0.0000000000000424488380\n",
      "roused\t0.0000000000000485875742\n",
      "cheerful\t0.0000000000000578933049\n",
      "seriously\t0.0000000000000622191045\n",
      "connected\t0.0000000000000645253336\n",
      "objection\t0.0000000000000755909497\n",
      "respectability\t0.0000000000000805060800\n",
      "income\t0.0000000000000836668355\n",
      "behaviour\t0.0000000000000889615188\n",
      "voluntarily\t0.0000000000000893861582\n",
      "receiving\t0.0000000000000916935935\n",
      "communication\t0.0000000000001019331138\n",
      "speak\t0.0000000000001040096685\n",
      "alteration\t0.0000000000001097798325\n",
      "disappointment\t0.0000000000001235234893\n",
      "concerns\t0.0000000000001395763634\n",
      "tolerable\t0.0000000000001484972122\n",
      "engaged\t0.0000000000001492551103\n",
      "occurred\t0.0000000000001546180255\n",
      "nash\t0.0000000000001582518728\n",
      "grieving\t0.0000000000001747974492\n",
      "combe\t0.0000000000001765203082\n",
      "lacey\t0.0000000000001779871197\n",
      "imagine\t0.0000000000001864799847\n",
      "mother\t0.0000000000001919517787\n",
      "gratified\t0.0000000000001985955699\n"
     ]
    }
   ],
   "source": [
    "print( \"The following words are most distinctive in corpus 1:\" )  \n",
    "\n",
    "i = 0\n",
    "max = 500\n",
    "\n",
    "for word in sortedByValue( rho ):\n",
    "    if rho[word] > 0:\n",
    "        print( f'{word}\\t{rho[word]:.22f}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are most distinctive in corpus 2 have a negative value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following words are most distinctive in corpus 2:\n",
      "upon: -0.0000000000000000000000\n",
      "lord: -0.0000000000000000000000\n",
      "old: -0.0000000000000000000000\n",
      "says: -0.0000000000000000000000\n",
      "ca: -0.0000000000000000000000\n",
      "men: -0.0000000000000000000001\n",
      "hath: -0.0000000000000000000006\n",
      "reader: -0.0000000000000000000007\n",
      "king: -0.0000000000000000000012\n",
      "french: -0.0000000000000000000022\n",
      "gentleman: -0.0000000000000000000207\n",
      "cries: -0.0000000000000000000629\n",
      "english: -0.0000000000000000001106\n",
      "arms: -0.0000000000000000001969\n",
      "master: -0.0000000000000000002874\n",
      "fellow: -0.0000000000000000005172\n",
      "man: -0.0000000000000000005436\n",
      "life: -0.0000000000000000013460\n",
      "boy: -0.0000000000000000017363\n",
      "order: -0.0000000000000000017621\n",
      "thou: -0.0000000000000000045887\n",
      "scarce: -0.0000000000000000072362\n",
      "thus: -0.0000000000000000115488\n",
      "honest: -0.0000000000000000157644\n",
      "wo: -0.0000000000000001474998\n",
      "hand: -0.0000000000000002253913\n",
      "art: -0.0000000000000003638055\n",
      "royal: -0.0000000000000006033914\n",
      "thee: -0.0000000000000008099392\n",
      "heaven: -0.0000000000000010025846\n",
      "passion: -0.0000000000000013462675\n",
      "prince: -0.0000000000000013553859\n",
      "noble: -0.0000000000000073176039\n",
      "de: -0.0000000000000089982973\n",
      "queen: -0.0000000000000129889243\n",
      "madame: -0.0000000000000137034478\n",
      "court: -0.0000000000000163980301\n",
      "majesty: -0.0000000000000216820329\n",
      "doctor: -0.0000000000000288813372\n",
      "major: -0.0000000000000335531251\n",
      "hands: -0.0000000000000340118306\n",
      "money: -0.0000000000000372990878\n",
      "thy: -0.0000000000000427404609\n",
      "god: -0.0000000000000439056185\n",
      "feet: -0.0000000000000464611667\n",
      "black: -0.0000000000000509207807\n",
      "madam: -0.0000000000000529760149\n",
      "city: -0.0000000000001155363363\n",
      "george: -0.0000000000001406176073\n",
      "wretch: -0.0000000000001578297102\n",
      "nevertheless: -0.0000000000001808567485\n",
      "soul: -0.0000000000003802880948\n",
      "story: -0.0000000000003814178400\n",
      "death: -0.0000000000004622690959\n",
      "duke: -0.0000000000005528966760\n",
      "worthy: -0.0000000000006904910077\n",
      "arthur: -0.0000000000011469765716\n",
      "caused: -0.0000000000012506174451\n",
      "face: -0.0000000000017359824643\n",
      "paris: -0.0000000000022409441174\n",
      "pen: -0.0000000000028691857560\n",
      "poor: -0.0000000000041682340331\n",
      "although: -0.0000000000044719606530\n",
      "author: -0.0000000000045758467848\n",
      "wicked: -0.0000000000069328276202\n",
      "pendennis: -0.0000000000085074626726\n",
      "innocent: -0.0000000000093788739276\n",
      "enemy: -0.0000000000097100665020\n",
      "answered: -0.0000000000292631097334\n",
      "jack: -0.0000000000434894579421\n",
      "lordship: -0.0000000000680652077054\n",
      "cast: -0.0000000000704481976816\n",
      "france: -0.0000000001113977237014\n",
      "army: -0.0000000001220547158698\n",
      "persons: -0.0000000001241903916326\n",
      "honor: -0.0000000001656270201380\n",
      "ears: -0.0000000001692819871700\n",
      "battle: -0.0000000001934661363283\n",
      "yet: -0.0000000001995064351102\n",
      "virtue: -0.0000000002102147865798\n",
      "lad: -0.0000000002168883008461\n",
      "history: -0.0000000002729119005699\n",
      "glass: -0.0000000003152234308781\n",
      "lovelace: -0.0000000003464865739885\n",
      "whilst: -0.0000000003608100350586\n",
      "warrington: -0.0000000003625840162927\n",
      "england: -0.0000000004285561189483\n",
      "religion: -0.0000000004862643394960\n",
      "die: -0.0000000006098627872096\n",
      "brave: -0.0000000007108765019091\n",
      "flung: -0.0000000007843809777540\n",
      "remarked: -0.0000000008978495437053\n",
      "show: -0.0000000009533066401024\n",
      "desired: -0.0000000010894574569794\n",
      "princess: -0.0000000011076284386845\n",
      "notwithstanding: -0.0000000011965819567700\n",
      "amelia: -0.0000000011981811145057\n",
      "rage: -0.0000000013459781373874\n",
      "fellows: -0.0000000014271150829142\n",
      "earth: -0.0000000014484892999916\n",
      "also: -0.0000000020154368577264\n",
      "divine: -0.0000000020264869917401\n",
      "took: -0.0000000025420207598737\n",
      "blood: -0.0000000031757322342973\n",
      "surely: -0.0000000035020706283401\n",
      "war: -0.0000000035692280648235\n",
      "red: -0.0000000043546974466054\n",
      "burney: -0.0000000048237075108370\n",
      "fatal: -0.0000000049140688823400\n",
      "knees: -0.0000000052275322449345\n",
      "rome: -0.0000000055049219966729\n",
      "harry: -0.0000000058825504111430\n",
      "pocket: -0.0000000059421833264109\n",
      "held: -0.0000000064438204432241\n",
      "big: -0.0000000065297742085613\n",
      "stories: -0.0000000066158511242770\n",
      "bosom: -0.0000000068787006345618\n",
      "sword: -0.0000000111515734874711\n",
      "bound: -0.0000000116337889058113\n",
      "serve: -0.0000000121414883752592\n",
      "page: -0.0000000122199040709226\n",
      "hundred: -0.0000000131634498001986\n",
      "utterly: -0.0000000132245639849929\n",
      "grand: -0.0000000133854116331194\n",
      "women: -0.0000000137866848920051\n",
      "gentlemen: -0.0000000156631451800508\n",
      "nose: -0.0000000162116439758749\n",
      "roman: -0.0000000168652682508608\n",
      "title: -0.0000000189685316211628\n",
      "condition: -0.0000000202164121866149\n",
      "tale: -0.0000000202287241302958\n",
      "let: -0.0000000202740200307449\n",
      "served: -0.0000000208280541060929\n",
      "scorn: -0.0000000213215601651845\n",
      "reputation: -0.0000000218399867383935\n",
      "folks: -0.0000000221075799590119\n",
      "night: -0.0000000228797658342569\n",
      "famous: -0.0000000233130304539002\n",
      "parson: -0.0000000248439348813702\n",
      "carried: -0.0000000259949911398866\n",
      "affairs: -0.0000000277357037152509\n",
      "esmond: -0.0000000283322612419055\n",
      "cecilia: -0.0000000294889094223566\n",
      "white: -0.0000000302589748470107\n",
      "accustomed: -0.0000000322169071744367\n",
      "booth: -0.0000000323441282414584\n",
      "permit: -0.0000000345515911077501\n",
      "coat: -0.0000000350861693077509\n",
      "europe: -0.0000000412631340068760\n",
      "fight: -0.0000000417902636367877\n",
      "bottle: -0.0000000418935489067907\n",
      "mouth: -0.0000000473441655756980\n",
      "government: -0.0000000492766574080065\n",
      "harlowe: -0.0000000598704251816206\n",
      "drink: -0.0000000600567190267752\n",
      "works: -0.0000000716531798323996\n",
      "regarding: -0.0000000724025369697849\n",
      "generous: -0.0000000750346653090226\n",
      "castle: -0.0000000774396533824205\n",
      "sacred: -0.0000000800544597695294\n",
      "honour: -0.0000000821202407125943\n",
      "doth: -0.0000000836280274312711\n",
      "dead: -0.0000000852810987656650\n",
      "reverence: -0.0000000884372849358140\n",
      "bill: -0.0000000911761037168585\n",
      "law: -0.0000000949555097338307\n",
      "went: -0.0000001107932268096936\n",
      "monsieur: -0.0000001156540699718014\n",
      "task: -0.0000001184165119847194\n",
      "according: -0.0000001204908710881497\n",
      "book: -0.0000001301373843464203\n",
      "british: -0.0000001470139957683890\n",
      "la: -0.0000001475607750522416\n",
      "called: -0.0000001574746063949889\n",
      "sum: -0.0000001600448226413325\n",
      "public: -0.0000001639695480005761\n",
      "readers: -0.0000001661039766062978\n",
      "jones: -0.0000001768567877294154\n",
      "pair: -0.0000001815134970540996\n",
      "fell: -0.0000001912332243656411\n",
      "wine: -0.0000002034354554847314\n",
      "presence: -0.0000002046252518885687\n",
      "howe: -0.0000002048031040731471\n",
      "us: -0.0000002051718425245574\n",
      "faithful: -0.0000002139627737285609\n",
      "clavering: -0.0000002236329163203579\n",
      "mamma: -0.0000002247064094322851\n",
      "pale: -0.0000002303084046820445\n",
      "lay: -0.0000002364295598690614\n",
      "position: -0.0000002426754520230300\n",
      "guilty: -0.0000002491386284221536\n",
      "camilla: -0.0000002840958010664221\n",
      "countess: -0.0000002847521076658242\n",
      "fear: -0.0000002944653494000814\n",
      "retired: -0.0000002961749957927486\n",
      "greek: -0.0000003168235036783831\n",
      "whither: -0.0000003235889681937630\n",
      "castlewood: -0.0000003316645931419101\n",
      "devil: -0.0000003731132660286920\n",
      "yonder: -0.0000003937984805349068\n",
      "military: -0.0000004329513340258907\n",
      "christian: -0.0000004348094420180991\n",
      "swore: -0.0000004447199889151848\n",
      "mighty: -0.0000004573496373918749\n",
      "boots: -0.0000004630406590904914\n",
      "minister: -0.0000004630918513647761\n",
      "race: -0.0000005301773472023371\n",
      "church: -0.0000005636977114056740\n",
      "concerning: -0.0000005919857126360167\n",
      "clarissa: -0.0000006111002577705328\n",
      "lie: -0.0000006303051951010760\n",
      "college: -0.0000006810881196648312\n",
      "filled: -0.0000006915006572237405\n",
      "enemies: -0.0000007513437570728849\n",
      "passions: -0.0000007935122811471867\n",
      "presently: -0.0000008268601620080028\n",
      "learned: -0.0000008385168521349374\n",
      "sate: -0.0000008654066405169078\n",
      "forth: -0.0000008845873034890301\n",
      "palace: -0.0000009346778531381117\n",
      "carry: -0.0000009517112863310188\n",
      "francis: -0.0000009882368102065513\n",
      "prisoner: -0.0000011140432712925491\n",
      "intend: -0.0000011668062232674460\n",
      "splendid: -0.0000011860324799521408\n",
      "blue: -0.0000012270399806554727\n",
      "wrath: -0.0000012802627602107789\n",
      "compelled: -0.0000013328644369568340\n",
      "ordered: -0.0000013846797768322708\n",
      "grief: -0.0000014063494329697345\n",
      "gold: -0.0000014406933689515307\n",
      "shilling: -0.0000014432623286594780\n",
      "begged: -0.0000014925049553638853\n",
      "pious: -0.0000015246014554329375\n",
      "land: -0.0000015266842128050570\n",
      "hat: -0.0000015457457856758612\n",
      "prison: -0.0000015464213065614140\n",
      "widow: -0.0000016277960381497636\n",
      "commands: -0.0000016530864071683883\n",
      "murder: -0.0000017660979596171976\n",
      "content: -0.0000017911173711927744\n",
      "save: -0.0000018877667141647029\n",
      "become: -0.0000019078127221897159\n",
      "goes: -0.0000019849279062267033\n",
      "soldier: -0.0000020593645274006884\n",
      "helen: -0.0000020744509859664783\n",
      "laws: -0.0000021186938337839507\n",
      "sorts: -0.0000021559628582382097\n",
      "cicero: -0.0000021676339567860954\n",
      "name: -0.0000022935872819265281\n",
      "lying: -0.0000023047927625288558\n",
      "nobleman: -0.0000023461680883324443\n",
      "coach: -0.0000023790162668520820\n",
      "cry: -0.0000025572705933402997\n",
      "hast: -0.0000025765365577025128\n",
      "apartment: -0.0000026017942052238004\n",
      "breast: -0.0000027304014933216683\n",
      "glory: -0.0000027477938761509383\n",
      "glorious: -0.0000027935958327581033\n",
      "earl: -0.0000028871597358328344\n",
      "whence: -0.0000029132189364228596\n",
      "mortal: -0.0000029193934062786814\n",
      "savage: -0.0000029256570760616724\n",
      "simply: -0.0000029374598647767063\n",
      "fury: -0.0000031767337344559136\n",
      "put: -0.0000032184546698423128\n",
      "papers: -0.0000032570741739997555\n",
      "youth: -0.0000033667223968863571\n",
      "tears: -0.0000034010390742374155\n",
      "poet: -0.0000034456397354620060\n",
      "political: -0.0000034648379387174602\n",
      "hotel: -0.0000035304367424818188\n",
      "admirable: -0.0000035657795364459426\n",
      "foreign: -0.0000035813133226706466\n",
      "presented: -0.0000035833454746214415\n",
      "mankind: -0.0000038072688155692184\n",
      "country: -0.0000041528162072358372\n",
      "belford: -0.0000041623011066891133\n",
      "broke: -0.0000042050230398445060\n",
      "break: -0.0000042776592317758065\n",
      "committed: -0.0000043065355645184562\n",
      "crime: -0.0000045622199655792812\n",
      "mine: -0.0000046326330395934944\n",
      "prayers: -0.0000046599232798548353\n",
      "somewhat: -0.0000048247651705611039\n",
      "painted: -0.0000049320718876891680\n",
      "humble: -0.0000052449194700633835\n",
      "sovereign: -0.0000053801799175627065\n",
      "published: -0.0000056097369204225271\n",
      "latin: -0.0000058466544385301769\n",
      "curse: -0.0000058593327651002126\n",
      "fly: -0.0000062122534835554958\n",
      "joseph: -0.0000064303966163740877\n",
      "bishop: -0.0000065345635042374901\n",
      "kissed: -0.0000066388950342749398\n",
      "read: -0.0000069506193999786379\n",
      "swear: -0.0000069825124931706923\n",
      "religious: -0.0000071453791809575789\n",
      "saint: -0.0000071768885581341862\n",
      "immense: -0.0000072237009476968118\n",
      "water: -0.0000076081354331214410\n",
      "sent: -0.0000077564025394806234\n",
      "lies: -0.0000079091608577344184\n",
      "laid: -0.0000079504565804309119\n",
      "officer: -0.0000081523397284443907\n",
      "genius: -0.0000084326399024435688\n",
      "highness: -0.0000087133747484384028\n",
      "le: -0.0000087134547409502679\n",
      "virtuous: -0.0000089591760304167718\n",
      "insisted: -0.0000090996439359725699\n",
      "hard: -0.0000094505946510655264\n",
      "louis: -0.0000095049774675712451\n",
      "hang: -0.0000098935239630246826\n",
      "favor: -0.0000099144201981033940\n",
      "native: -0.0000101238414901235769\n",
      "behold: -0.0000101794912234178250\n",
      "foot: -0.0000104204792452497710\n",
      "johnson: -0.0000104356674556243133\n",
      "conceived: -0.0000106272700004695124\n",
      "pieces: -0.0000107518369247758278\n",
      "hanged: -0.0000108133257208706563\n",
      "beer: -0.0000108204514493703079\n",
      "dark: -0.0000108876090537128869\n",
      "club: -0.0000116714289514267575\n",
      "work: -0.0000118252181772769578\n",
      "fought: -0.0000122925469734460540\n",
      "huge: -0.0000123170199167069733\n",
      "villain: -0.0000125863071046301930\n",
      "confounded: -0.0000125965930188023067\n",
      "misfortunes: -0.0000132926752783380950\n",
      "pay: -0.0000134619089937794835\n",
      "aid: -0.0000135597634779420511\n",
      "dog: -0.0000136091546340330712\n",
      "goodness: -0.0000138591287902697025\n",
      "profound: -0.0000140322111175102523\n",
      "rogue: -0.0000140365889012715595\n",
      "neck: -0.0000142231002758204612\n",
      "acquaint: -0.0000143381486370676679\n",
      "reasons: -0.0000144685968355099226\n",
      "fierce: -0.0000146548008310318010\n",
      "descended: -0.0000150148335000829031\n",
      "cruel: -0.0000152386517712294781\n",
      "sedley: -0.0000153312090824087209\n",
      "lonely: -0.0000157253161759828317\n",
      "incident: -0.0000159824344938305634\n",
      "monster: -0.0000159895805024928215\n",
      "republic: -0.0000160054756052468023\n",
      "virtues: -0.0000161193707438888557\n",
      "shoulder: -0.0000162190496190656702\n",
      "nation: -0.0000165984346958158345\n",
      "pound: -0.0000167042348728933084\n",
      "human: -0.0000170460991388136601\n",
      "german: -0.0000178484132501921152\n",
      "duchess: -0.0000180488588032579773\n",
      "retreat: -0.0000188147138391853125\n",
      "governor: -0.0000189117835051640573\n",
      "troops: -0.0000190340158146245879\n",
      "laughter: -0.0000190941438981432490\n",
      "bless: -0.0000195450610972184372\n",
      "demanded: -0.0000197055280707974798\n",
      "passionate: -0.0000198534534337120402\n",
      "special: -0.0000202976550994214721\n",
      "fate: -0.0000208556100717917086\n",
      "rode: -0.0000213044071684731639\n",
      "row: -0.0000216155392095088324\n",
      "terrible: -0.0000225590982606786586\n",
      "delvile: -0.0000226614569462896227\n",
      "mercy: -0.0000247930462054409639\n",
      "squire: -0.0000250902460598101703\n",
      "cup: -0.0000268905895817175384\n",
      "obedience: -0.0000269124643500008833\n",
      "action: -0.0000272807476944338968\n",
      "accident: -0.0000291755823674490824\n",
      "showed: -0.0000291966915146083042\n",
      "hero: -0.0000292339617058477083\n",
      "besought: -0.0000293495603294612219\n",
      "drank: -0.0000294466284490468397\n",
      "commonly: -0.0000294947216911714868\n",
      "cursed: -0.0000298185758346878293\n",
      "wilt: -0.0000306868510167251452\n",
      "new: -0.0000310903891501921819\n",
      "seized: -0.0000312936241060574971\n",
      "vowed: -0.0000314699367290406412\n",
      "eternal: -0.0000328298713256238827\n",
      "piety: -0.0000333605293900108245\n",
      "golden: -0.0000334435370568105680\n",
      "b: -0.0000334656175238833050\n",
      "tremendous: -0.0000349172082864272339\n",
      "legs: -0.0000354864245839055489\n",
      "et: -0.0000362079727169023672\n",
      "abroad: -0.0000374421728316546123\n",
      "bed: -0.0000377124945717156472\n",
      "victim: -0.0000380484112796403482\n",
      "fairoaks: -0.0000381308691526611136\n",
      "fashion: -0.0000385832258066305889\n",
      "irish: -0.0000388403036851266750\n",
      "guineas: -0.0000410759296924402019\n",
      "goods: -0.0000415605055610604923\n",
      "wisdom: -0.0000424150172470742315\n",
      "bade: -0.0000424310703970229015\n",
      "waters: -0.0000424524492623337010\n",
      "calls: -0.0000431392192826929167\n",
      "base: -0.0000433530915172120518\n",
      "weeping: -0.0000433908071196504482\n",
      "hold: -0.0000442158131831238475\n",
      "betty: -0.0000458911034079982202\n",
      "forehead: -0.0000473478941189982270\n",
      "blanche: -0.0000473838592111577853\n",
      "beat: -0.0000476110748289132026\n",
      "member: -0.0000476882683547159934\n",
      "angel: -0.0000480023854445859154\n",
      "bills: -0.0000485952967835686728\n",
      "soldiers: -0.0000490543158563797220\n",
      "free: -0.0000491671269649912062\n",
      "innocence: -0.0000492517034024531864\n",
      "novel: -0.0000492706033681823967\n",
      "fighting: -0.0000492804887334968652\n",
      "writers: -0.0000492857941109663968\n",
      "dismal: -0.0000499651623533567719\n",
      "drunk: -0.0000515787233951683858\n",
      "freedom: -0.0000516507383114410591\n",
      "discovered: -0.0000527156956474183309\n",
      "whereas: -0.0000533741178119108084\n",
      "count: -0.0000535795043010692707\n",
      "dost: -0.0000539388159353505590\n",
      "fool: -0.0000552987116375411094\n",
      "englishman: -0.0000560499587763710723\n",
      "moon: -0.0000561251611456097300\n",
      "jolly: -0.0000563087795297150467\n",
      "teeth: -0.0000570015777176673572\n",
      "knows: -0.0000574212253071194491\n",
      "pardon: -0.0000580971093958503526\n",
      "holding: -0.0000601897490094543833\n",
      "expedition: -0.0000602900468665271187\n",
      "alas: -0.0000612883694776460695\n",
      "empire: -0.0000614234454823220496\n",
      "emperor: -0.0000614571905293519302\n",
      "eugenia: -0.0000614784968576595855\n",
      "pulled: -0.0000622359115209840348\n",
      "shoulders: -0.0000624030076682003900\n",
      "victory: -0.0000639316861992057149\n",
      "tyrold: -0.0000642008677304613663\n",
      "flowers: -0.0000642054199995022038\n",
      "permitted: -0.0000650975773565657312\n",
      "chamber: -0.0000665610145799236283\n",
      "reverend: -0.0000669934057711249801\n",
      "sin: -0.0000690920594931383704\n",
      "copy: -0.0000698902014889646967\n",
      "favorite: -0.0000699635219977070704\n",
      "covered: -0.0000726775643755638430\n",
      "wept: -0.0000734423466488502285\n",
      "sleep: -0.0000736392244502046965\n",
      "holy: -0.0000760748875289529903\n",
      "fain: -0.0000762757532836313512\n",
      "commenced: -0.0000762851517675277365\n",
      "kiss: -0.0000771353117463891464\n",
      "soft: -0.0000782658957108706377\n",
      "pure: -0.0000790336865621515185\n",
      "marquis: -0.0000793052318276402557\n",
      "possessed: -0.0000794575175658433953\n",
      "silver: -0.0000807677523604031659\n",
      "porter: -0.0000811846505315075284\n",
      "celebrated: -0.0000827128369948504258\n",
      "purposes: -0.0000829948344145839487\n",
      "fled: -0.0000830798592158571631\n",
      "tutor: -0.0000832605271459044955\n",
      "hugh: -0.0000832963454848662031\n",
      "literary: -0.0000836625817553053190\n",
      "rascal: -0.0000839686525209863471\n",
      "revenge: -0.0000859833596665518106\n",
      "university: -0.0000863031863247832562\n",
      "worship: -0.0000888184170757259819\n",
      "exalted: -0.0000907492606110187083\n",
      "darling: -0.0000921214192207835293\n",
      "grace: -0.0000926119706220815810\n",
      "vengeance: -0.0000948138810668830548\n",
      "virginia: -0.0000948540758583717186\n",
      "voyage: -0.0000965146119601082298\n",
      "opera: -0.0000985812895409569353\n",
      "abandoned: -0.0000989836947460192700\n",
      "humbly: -0.0001033841058185370636\n",
      "crawley: -0.0001034927658164762782\n",
      "asks: -0.0001036194424506108562\n",
      "ran: -0.0001064147094138210778\n",
      "verses: -0.0001070839037935479815\n",
      "bar: -0.0001076858490049902495\n",
      "tyrant: -0.0001079821871839958532\n",
      "scene: -0.0001088619088242974350\n",
      "wig: -0.0001123463134180281280\n",
      "namely: -0.0001127571323884409400\n",
      "writ: -0.0001128133988108630015\n",
      "clean: -0.0001133084214223652051\n",
      "literature: -0.0001152253757315234721\n",
      "pipe: -0.0001173272603019285409\n",
      "crying: -0.0001175733760287331965\n",
      "applause: -0.0001177992005626534957\n",
      "juliet: -0.0001178884597979491388\n",
      "violent: -0.0001224130077914916900\n",
      "bell: -0.0001256815257683715939\n",
      "thousand: -0.0001267076973038439437\n"
     ]
    }
   ],
   "source": [
    "print( \"The following words are most distinctive in corpus 2:\"  )  \n",
    "\n",
    "i = 0\n",
    "max = 500\n",
    "\n",
    "for word in sortedByValue( rho , ascending = False ) :\n",
    "    if rho[word] < 0:\n",
    "        print( f'{word}: {rho[word]:.22f}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* Dunning, Ted, 'Accurate Methods for the Statistics of Surprise and Coincidence', in *Computational Linguistics*, 19:1 (1993).\n",
    "* Rayson, P. and Garside, R., 'Comparing corpora using frequency profiling', in *Proceedings of the workshop on Comparing Corpora, held in conjunction with the 38th annual meeting of the Association for Computational Linguistics (ACL 2000)* (2000)\n",
    "* H. Mann and D. Whitney, 'On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other', in *Ann. Math. Statist.*, 1:18 (1947). <https://doi.org/10.1214/aoms/1177730491>\n",
    "* Adam Kilgarriff, *Comparing Corpora*, in *International Journal of Corpus Linguistics*, 6:1 (2001). <https://doi.org/10.1075/ijcl.6.1.05kil>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise 9.1\n",
    "\n",
    "Can you compare the diction of *Pride and Prejudice* using the Mann Whitney formula?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Corpus'\n",
    "\n",
    "corpus1 = [ 'PrideandPrejudice.txt' ]\n",
    "corpus2 = [ 'Ulysses.txt' ]\n",
    "\n",
    "\n",
    "def tokenise_remove_stopwords(full_text):\n",
    "    words = word_tokenize(full_text)\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        w = w.lower().strip()\n",
    "        orig = ''\n",
    "        if w.isalnum() and w not in stopwords:\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "\n",
    "\n",
    "full_text1 = ''\n",
    "full_text2 = ''\n",
    "\n",
    "for text in corpus1:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir,text) ) as file_handler:\n",
    "        full_text1 += file_handler.read() + ' '\n",
    "\n",
    "for text in corpus2:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir,text) ) as file_handler:\n",
    "        full_text2 += file_handler.read() + ' '\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "## make a list of all the words in both corpora\n",
    "words1 = tokenise_remove_stopwords(full_text1)\n",
    "words2 = tokenise_remove_stopwords(full_text2)\n",
    "\n",
    "def divide_into_chunks(words, length):\n",
    "\n",
    "    chunks=[]\n",
    "    ## chunk contains dictionaries\n",
    "    # with word frequencies\n",
    "    \n",
    "    for i in range(0, len(words), length):\n",
    "        counts = dict()\n",
    "        for j in range(length):\n",
    "            if i+j < len(words):\n",
    "                word = words[i+j]\n",
    "                counts[word] = counts.get(word,0)+1\n",
    "        chunks.append(counts)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "length = 500\n",
    "chunks1 = divide_into_chunks(words1,length)\n",
    "chunks2 = divide_into_chunks(words2,length)\n",
    "\n",
    "\n",
    "# vocab is the union of terms in both sets\n",
    "all_words = dict()\n",
    "    \n",
    "for chunk in chunks1:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "for chunk in chunks2:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "    \n",
    "rho =  dict()\n",
    "    \n",
    "for word in all_words:\n",
    "        \n",
    "    a=[]\n",
    "    b=[]\n",
    "        \n",
    "    for chunk in chunks1:\n",
    "        a.append(chunk.get(word,0))\n",
    "    for chunk in chunks2:\n",
    "        b.append(chunk.get(word,0))\n",
    "\n",
    "    stat,pval=mannwhitneyu(a,b, alternative=\"two-sided\")\n",
    "    mean =len(chunks1)*len(chunks2)*0.5\n",
    "    if stat <= mean:\n",
    "        pval = 0 - pval\n",
    "            \n",
    "    rho[word]= ( pval )\n",
    "    \n",
    "print( f\"\\nThe following words are most distinctive in {corpus1}\" )  \n",
    "\n",
    "i = 0\n",
    "max = 25\n",
    "\n",
    "for word in sortedByValue( rho ):\n",
    "    if rho[word] > 0:\n",
    "        print( f'{word}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break\n",
    "            \n",
    "\n",
    "print( f\"\\nThe following words are most distinctive in {corpus2}\" )  \n",
    "\n",
    "i = 0\n",
    "max = 25\n",
    "\n",
    "for word in sortedByValue( rho , ascending = False ) :\n",
    "    if rho[word] < 0:\n",
    "        print( f'{word}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
